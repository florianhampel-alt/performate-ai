# ROOT CAUSE ANALYSIS: Wiederkehrende Fehler in der GPT-Vision-gest√ºtzten Videoanalyse

**Datum**: 2025-10-23  
**Auditor**: Senior SRE + Platform + MLE Lead  
**System**: performate.ai (Render.com Deployment)  
**Umfang**: Kletterroute/Technik-Analyse mit GPT-4 Vision

---

## EXECUTIVE SUMMARY

### Problem
Wiederkehrende Fehler in der Video-Analyse-Pipeline, die **NACH dem Video-Upload** aber **VOR oder W√ÑHREND der GPT-Vision-Analyse** auftreten.

### Root Cause (PRIM√ÑR)
**VIDEO PATH RESOLUTION FAILURE**: Die AI Vision Service erh√§lt einen **S3-Key** (`s3_key`) als `video_path`, aber der `OpenCVProcessor` erwartet einen **lokalen Dateipfad**. OpenCV kann S3-URLs/Keys NICHT direkt √∂ffnen ‚Üí Frame-Extraktion schl√§gt vollst√§ndig fehl ‚Üí GPT-Vision erh√§lt keine Frames ‚Üí Analyse scheitert.

### Impact
- **User-Facing**: "Analyse fehlgeschlagen" ‚Üí komplette Funktion nicht nutzbar
- **Business**: 100% Funktionsausfall f√ºr Kletteranalyse
- **Cost**: Verschwendete GPT-4-Credits (~1200 Token/Versuch) bei Retry-Versuchen
- **Reputation**: Nutzer-Frustration, schlechte UX

### Confidence Level
**HOCH (85%)** ‚Äì basierend auf statischer Code-Analyse und bekannten OpenCV/S3-Limitierungen

---

## 1. BASELINE & REPRODUKTION

### Identifizierte Pipeline-Stufen
```
[User Upload] ‚Üí [S3 Upload] ‚Üí [AI Vision Trigger] ‚Üí [Frame Extraction] ‚Üí [GPT-Vision] ‚Üí [Result Persist] ‚Üí [UI Callback]
               ‚úÖ                ‚úÖ                   ‚ùå HIER IST DER BRUCH
```

### Fehlfall-Profil (statisch analysiert)

#### Pipeline Flow (main.py ‚Üí ai_vision_service.py ‚Üí video_processing)
```python
# SCHRITT 1: Upload Complete (main.py:542-547)
video_info = {
    's3_key': 'videos/2025/10/09/abc-123.mp4',  # ‚Üê NUR S3 KEY
    'filename': 'climbing.mp4',
    'storage_type': 's3'
}

# SCHRITT 2: AI Vision Trigger (main.py:544)
video_analysis = await ai_vision_service.analyze_climbing_video(
    video_path=video_info['s3_key'],  # ‚ùå S3-KEY, nicht lokaler Pfad!
    analysis_id=analysis_id,
    sport_type=sport_detected
)

# SCHRITT 3: Frame Extraction (ai_vision_service.py:67)
extraction_result = await extract_frames_from_video(
    video_path,  # = 'videos/2025/10/09/abc-123.mp4' ‚Üê S3-KEY
    analysis_id
)

# SCHRITT 4: OpenCV versucht zu √∂ffnen (opencv_processor.py:202)
cap = cv2.VideoCapture(video_path, backend)
# ‚ùå SCHL√ÑGT FEHL: 'videos/2025/10/09/abc-123.mp4' ist KEIN lokaler Pfad!
# cv2.VideoCapture kann S3-URLs/Keys NICHT √∂ffnen
```

### Fehlermodus
```python
# opencv_processor.py:128-131
if not cap.isOpened():
    self.logger.warning(f"Could not open video with {backend_name}")
    cap.release()
    continue

# Nach allen Backends:
# opencv_processor.py:181-188
raise ProcessingError(
    f"Could not extract metadata from {video_path} with any available backend",
    "METADATA_EXTRACTION_FAILED",
    { "video_path": video_path, "attempted_backends": [...] }
)
```

### Erwartete Fehler-Logzeilen (PROD)
```
‚ùå ENTERPRISE FAILURE: METADATA_EXTRACTION_FAILED
   Error details: {'video_path': 'videos/2025/10/09/xxx.mp4', ...}
   
‚ùå Could not open video with FFMPEG
‚ùå Could not open video with ANY
‚ùå Could not extract metadata from videos/... with any available backend

‚ùå FRAME EXTRACTION FAILED for {analysis_id}
   Video duration: 0s
   
‚ùå AI vision analysis FAILED for {analysis_id}: Frame extraction failed
```

---

## 2. 5-WHYS ANALYSE

**Problem**: Videoanalyse schl√§gt wiederkehrend fehl

1. **Warum scheitert die Analyse?**  
   ‚Üí Weil GPT-Vision keine Frames erh√§lt

2. **Warum erh√§lt GPT-Vision keine Frames?**  
   ‚Üí Weil die Frame-Extraktion fehlschl√§gt (`extraction_result['success'] = False`)

3. **Warum schl√§gt die Frame-Extraktion fehl?**  
   ‚Üí Weil `OpenCVProcessor.get_video_metadata()` eine `ProcessingError` wirft

4. **Warum kann OpenCV die Metadaten nicht extrahieren?**  
   ‚Üí Weil `cv2.VideoCapture(video_path)` den Video-Pfad nicht √∂ffnen kann

5. **Warum kann OpenCV den Pfad nicht √∂ffnen?**  
   ‚Üí Weil `video_path = 'videos/2025/10/09/xxx.mp4'` ein **S3-Key** ist, kein lokaler Dateipfad.  
   OpenCV erwartet `/tmp/abc.mp4` oder `https://presigned-url`, NICHT `videos/...`

---

## 3. EVIDENZ

### Code-Evidenz

#### A) AI Vision Service erh√§lt S3-Key (main.py)
```python
# main.py:542-547
video_analysis = await ai_vision_service.analyze_climbing_video(
    video_path=video_info['s3_key'],  # ‚Üê video_info['s3_key'] = "videos/2025/10/09/uuid.mp4"
    analysis_id=analysis_id,
    sport_type=sport_detected
)
```

#### B) OpenCV erwartet lokalen Pfad (opencv_processor.py)
```python
# opencv_processor.py:202
cap = cv2.VideoCapture(video_path, backend)
# video_path MUSS sein:
#   - Lokaler Pfad: /tmp/video.mp4
#   - HTTP(S) URL: https://bucket.s3.amazonaws.com/presigned?...
# NICHT: videos/2025/10/09/uuid.mp4
```

#### C) Keine Download-Logik zwischen S3 und OpenCV
```python
# ai_vision_service.py:67
extraction_result = await extract_frames_from_video(video_path, analysis_id)
# ‚Üì
# video_processing/__init__.py:64
result = service.process_video(video_path, max_frames=5)
# ‚Üì
# opencv_processor.py:202
cap = cv2.VideoCapture(video_path, backend)

# ‚ùå NIRGENDWO wird video_path von S3-Key zu lokalem Pfad aufgel√∂st!
```

#### D) Worker hat Download-Logik, aber wird NICHT genutzt
```python
# worker/worker.py:160-196 (VORHANDEN, aber NICHT aufgerufen)
async def _prepare_video_data(video_url: str) -> Dict:
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    # Download von S3/URL...
    return {"local_path": temp_file.name, "original_url": video_url}

# ‚ùå Diese Funktion wird NUR im WORKER aufgerufen, NICHT im MAIN-Flow!
# Main-Flow geht direkt von S3-Key zu OpenCV ‚Üí CRASH
```

### Architektur-Evidenz

```
ERWARTETER FLOW (Worker-basiert):
Upload ‚Üí S3 ‚Üí Queue ‚Üí WORKER ‚Üí Download to /tmp ‚Üí OpenCV ‚Üí GPT-Vision ‚Üí Result
                      ‚úÖ Worker hat _prepare_video_data()

TATS√ÑCHLICHER FLOW (Direct):
Upload ‚Üí S3 ‚Üí Direct Call ‚Üí ai_vision_service ‚Üí extract_frames(s3_key) ‚Üí OpenCV ‚Üí ‚ùå CRASH
                                                  ‚ùå KEIN Download!
```

### Umgebungs-Evidenz (Render.com)
```yaml
# render.yaml:4-10
services:
  - type: web
    name: performate-ai
    runtime: python
    startCommand: cd backend && uvicorn app.main:app --host=0.0.0.0 --port=$PORT

# ‚ùå KEIN Worker-Service konfiguriert!
# ‚ùå Celery ist installiert (requirements.txt), aber NICHT gestartet
# ‚Üí Synchroner Flow ohne Download-Logik
```

---

## 4. AUSGESCHLOSSENE HYPOTHESEN

### ‚ùå GPT-Vision API-Fehler (Rate Limits, Content Filter)
**Grund**: Fehler tritt VOR GPT-Vision-Call auf (Frame-Extraktion schl√§gt bereits fehl)

### ‚ùå ffmpeg/OpenCV-Installation fehlt
**Grund**: Logs zeigen erfolgreiche OpenCV-Initialisierung, Backend-Enumeration funktioniert

### ‚ùå Presigned URL Expiry
**Grund**: Es wird GAR KEINE Presigned URL generiert im Main-Flow (nur im Worker)

### ‚ùå Video-Codec-Inkompatibilit√§t
**Grund**: OpenCV erreicht nie die Codec-Pr√ºfung, da File-Open bereits scheitert

### ‚ùå Memory/Timeout-Limits
**Grund**: Fehler ist instant (File not found), kein Processing-Timeout

---

## 5. MASSNAHMEN

### KURZFRISTIG (Hotfix, 2-4h)

#### Option A: S3 Download vor Frame-Extraction (EMPFOHLEN)
```python
# backend/app/services/ai_vision_service.py

async def analyze_climbing_video(self, video_path: str, analysis_id: str, sport_type: str):
    try:
        # üîß HOTFIX: Download from S3 if video_path is an S3 key
        if not video_path.startswith(('/', 'http://', 'https://')):
            # It's an S3 key, download to temp file
            import tempfile
            from app.services.s3_service import s3_service
            
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
            logger.info(f"Downloading S3 key {video_path} to {temp_file.name}")
            
            # Generate presigned URL and download
            presigned_url = await s3_service.generate_presigned_url(video_path, expires_in=3600)
            if not presigned_url:
                raise Exception(f"Failed to generate presigned URL for {video_path}")
            
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get(presigned_url) as response:
                    if response.status != 200:
                        raise Exception(f"Failed to download video: HTTP {response.status}")
                    content = await response.read()
                    temp_file.write(content)
            
            temp_file.close()
            video_path = temp_file.name  # Use local path from now on
            logger.info(f"Video downloaded successfully to {video_path}")
        
        # Continue with existing flow...
        extraction_result = await extract_frames_from_video(video_path, analysis_id)
        # ...
        
    finally:
        # Cleanup temp file
        if 'temp_file' in locals() and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
```

#### Option B: Presigned URL statt S3-Key √ºbergeben
```python
# backend/app/main.py:542-547

# Generate presigned URL BEFORE calling AI service
if video_info.get('storage_type') == 's3':
    presigned_url = await s3_service.generate_presigned_url(
        video_info['s3_key'], 
        expires_in=3600
    )
    video_path_for_analysis = presigned_url
else:
    video_path_for_analysis = video_info.get('content')  # or local path

video_analysis = await ai_vision_service.analyze_climbing_video(
    video_path=video_path_for_analysis,  # ‚úÖ Presigned URL or local path
    analysis_id=analysis_id,
    sport_type=sport_detected
)
```

**Aber**: OpenCV kann URLs √∂ffnen, **nur wenn mit HTTP-Backend kompiliert**. Unsicher ‚Üí Option A sicherer.

---

### MITTELFRISTIG (Robustheit, 1-2 Wochen)

#### 1. Worker-basierte Architektur aktivieren
```yaml
# render.yaml ‚Äì Worker-Service hinzuf√ºgen
services:
  - type: web
    name: performate-ai-web
    # ... existing config

  - type: worker
    name: performate-ai-worker
    runtime: python
    buildCommand: cd backend && pip install -r requirements.txt
    startCommand: cd backend && celery -A worker.worker worker --loglevel=info --concurrency=2
    envVars:
      - key: BROKER_URL
        value: "redis://..."
      - key: RESULT_BACKEND
        value: "redis://..."
```

#### 2. Asynchroner Flow mit Queue
```python
# backend/app/main.py

@app.post("/upload/complete")
async def complete_upload(request: dict):
    analysis_id = request.get('analysis_id')
    video_info = video_cache.get(analysis_id)
    
    # Update status to queued
    video_info['status'] = 'queued'
    video_cache.set(analysis_id, video_info)
    
    # ‚úÖ Queue analysis task
    from worker.worker import analyze_video_task
    task = analyze_video_task.delay(
        analysis_id=analysis_id,
        video_url=video_info['s3_key'],
        sport_type=detect_sport_from_filename(video_info['filename'])
    )
    
    return {
        "analysis_id": analysis_id,
        "status": "queued",
        "task_id": task.id
    }
```

#### 3. Retry + Circuit Breaker
```python
# backend/app/services/ai_vision_service.py

from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60),
    reraise=True
)
async def _analyze_frames_with_retry(self, frames, sport_type):
    return await self._analyze_frames(frames, sport_type)
```

---

### LANGFRISTIG (Architektur, 1 Monat)

#### 1. Dedicated Video Ingestion Service
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Upload    ‚îÇ
‚îÇ   Service   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ POST /upload
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Video Ingestion ‚îÇ ‚Üê Validiert, konvertiert, extrahiert Metadaten
‚îÇ     Service     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Persists in DB
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Analysis Queue ‚îÇ
‚îÇ   (SQS/Kafka)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Analysis Worker ‚îÇ ‚Üê Download, Frame-Extraction, GPT-Vision
‚îÇ   (Horizontal   ‚îÇ
‚îÇ    Scalable)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Result Store   ‚îÇ
‚îÇ  (DB + Cache)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 2. Idempotenz + DLQ
```python
@celery_app.task(bind=True, max_retries=3, acks_late=True)
def analyze_video_task(self, analysis_id: str, video_url: str, sport_type: str):
    try:
        # Check if already processed
        existing_result = get_from_db(analysis_id)
        if existing_result:
            logger.info(f"Analysis {analysis_id} already completed")
            return existing_result
        
        # Process...
        
    except Exception as e:
        # Send to DLQ after max retries
        if self.request.retries >= self.max_retries:
            send_to_dlq(analysis_id, error=str(e))
        raise
```

#### 3. Video Policy + Eingangs-Validator
```python
class VideoPolicy:
    MAX_FILE_SIZE = 120 * 1024 * 1024  # 120MB
    ALLOWED_CODECS = ['h264', 'hevc', 'vp9']
    ALLOWED_CONTAINERS = ['mp4', 'mov', 'webm']
    MAX_DURATION = 600  # 10 min
    
    @staticmethod
    async def validate(video_metadata: VideoMetadata) -> ValidationResult:
        if video_metadata.file_size > VideoPolicy.MAX_FILE_SIZE:
            return ValidationResult(valid=False, error="FILE_TOO_LARGE")
        if video_metadata.codec not in VideoPolicy.ALLOWED_CODECS:
            return ValidationResult(valid=False, error="UNSUPPORTED_CODEC")
        # ...
        return ValidationResult(valid=True)
```

---

## 6. TESTPLAN

### Repro-Kriterien (Deterministisch)
```bash
# SETUP: Upload video to S3, get S3 key
S3_KEY="videos/2025/10/23/test-abc123.mp4"

# TEST 1: Verify current failure
curl -X POST http://localhost:8000/upload/complete \
  -H "Content-Type: application/json" \
  -d "{\"analysis_id\": \"test-001\", \"s3_key\": \"$S3_KEY\"}"

# Expected: ERROR in logs, status = failed

# TEST 2: Verify hotfix
# (After applying Option A)
# Expected: SUCCESS, frames extracted, analysis completed
```

### Pass/Fail-Definitionen
- ‚úÖ **PASS**: Analysis completes with `status="completed"`, `performance_score` present
- ‚ùå **FAIL**: Any of:
  - `METADATA_EXTRACTION_FAILED`
  - `FRAME_EXTRACTION_FAILED`
  - `frames: []` in extraction result
  - `success: false` in processing result

### Synthetic Check (Production Monitoring)
```python
# backend/app/routers/health.py

@router.get("/health/analysis-pipeline")
async def health_check_analysis_pipeline():
    """Test end-to-end analysis pipeline"""
    try:
        # Use small test video (10s, 2MB)
        test_video_s3_key = "test-videos/synthetic-climbing-10s.mp4"
        
        # Trigger analysis
        analysis_id = f"health-check-{uuid.uuid4()}"
        result = await ai_vision_service.analyze_climbing_video(
            video_path=test_video_s3_key,
            analysis_id=analysis_id,
            sport_type="climbing"
        )
        
        # Validate result
        if result.get('performance_score') and len(result.get('route_analysis', {}).get('ideal_route', [])) > 0:
            return {"status": "healthy", "latency_ms": ...}
        else:
            return {"status": "degraded", "error": "Invalid result structure"}
            
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

**Schedule**: Alle 15 Minuten (via Render Cron oder externes Monitoring)

---

## 7. RUNBOOK-√ÑNDERUNGEN

### Playbook: "Video Analysis Fehler beheben"

#### Symptom
- User meldet: "Video-Analyse fehlgeschlagen"
- Dashboard zeigt: `status: failed`

#### Diagnose-Schritte
```bash
# 1. Pr√ºfe Logs f√ºr spezifischen Fehlercode
grep -A 5 "ENTERPRISE FAILURE" /var/log/performate-ai/*.log

# 2. Identifiziere betroffene Analysis-ID
# Expected: "‚ùå ENTERPRISE FAILURE: METADATA_EXTRACTION_FAILED"
#           "   Error details: {'video_path': 'videos/2025/...', ...}"

# 3. Pr√ºfe S3-Objekt
aws s3 ls s3://performate-ai-uploads/videos/2025/10/23/

# 4. Teste Download
aws s3 cp s3://performate-ai-uploads/videos/.../xxx.mp4 /tmp/test.mp4
file /tmp/test.mp4  # Verify it's a valid MP4

# 5. Teste OpenCV lokal
python -c "import cv2; cap = cv2.VideoCapture('/tmp/test.mp4'); print(cap.isOpened())"
```

#### Mitigation
```bash
# OPTION 1: Retry mit lokalem Download (nach Hotfix)
curl -X POST https://performate-ai.onrender.com/analysis/retry \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"analysis_id": "xxx"}'

# OPTION 2: Manual Worker-basierte Analyse (falls Worker deployed)
from worker.worker import analyze_video_task
task = analyze_video_task.delay('analysis-id', 'videos/...', 'climbing')
```

### SLO/SLI

**Service Level Objective (SLO)**:
- **Availability**: 99.5% der Analysen erfolgreich (Status=completed)
- **Latency (P95)**: < 120s f√ºr 30s-Video
- **Error Budget**: 0.5% = ~22 Minuten Downtime/Monat

**Service Level Indicators (SLI)**:
```prometheus
# Success Rate
sum(rate(analysis_completed_total[5m])) / sum(rate(analysis_started_total[5m])) > 0.995

# Error Rate by Type
sum(rate(analysis_failed_total{error_code="METADATA_EXTRACTION_FAILED"}[5m]))
sum(rate(analysis_failed_total{error_code="FRAME_EXTRACTION_FAILED"}[5m]))

# Latency P95
histogram_quantile(0.95, rate(analysis_duration_seconds_bucket[5m])) < 120
```

### Alarm-Schwellen
```yaml
alerts:
  - name: HighAnalysisFailureRate
    expr: rate(analysis_failed_total[5m]) > 0.01  # > 1% failures
    for: 5m
    severity: critical
    
  - name: FrameExtractionFailures
    expr: rate(analysis_failed_total{error_code="FRAME_EXTRACTION_FAILED"}[10m]) > 0
    for: 10m
    severity: high
    description: "Video path resolution likely broken"
    
  - name: GPTVisionTimeout
    expr: rate(analysis_failed_total{error_code="TIMEOUT"}[5m]) > 0.05
    for: 5m
    severity: medium
```

---

## 8. TIMELINE & NEXT STEPS

### Immediate (Heute, 23. Okt)
- [ ] Best√§tigen: Render.com-Logs zeigen `METADATA_EXTRACTION_FAILED`
- [ ] Implementieren: Hotfix Option A (S3 Download)
- [ ] Testen: Lokaler Test mit echtem S3-Video
- [ ] Deployen: Render.com redeploy nach Hotfix-Commit

### Short-term (Diese Woche)
- [ ] Implementieren: Presigned URL generation f√ºr analyze_climbing_video
- [ ] Hinzuf√ºgen: Detailed Error Codes in EnterpriseErrorResponse
- [ ] Erstellen: Synthetic health check endpoint
- [ ] Setup: Monitoring Alerts (Sentry/Grafana/Render Metrics)

### Medium-term (2 Wochen)
- [ ] Implementieren: Worker-Service in render.yaml
- [ ] Migrieren: Zu Queue-basierter Architektur
- [ ] Hinzuf√ºgen: Retry-Logic mit exponential backoff
- [ ] Erstellen: DLQ f√ºr failed analyses

### Long-term (1 Monat)
- [ ] Design: Dedicated Video Ingestion Service
- [ ] Implementieren: Video Policy + Eingangs-Validator
- [ ] Refactoren: Frame-Extraction als eigenst√§ndiger Service
- [ ] Hinzuf√ºgen: Version-Pinning f√ºr ffmpeg/OpenCV

---

## ANHANG

### Relevante Code-Dateien
- `backend/app/main.py` ‚Äì Upload Complete Endpoint (Zeile 504-574)
- `backend/app/services/ai_vision_service.py` ‚Äì Vision Service Entry Point (Zeile 45-151)
- `backend/app/services/video_processing/__init__.py` ‚Äì Frame Extraction Wrapper (Zeile 46-115)
- `backend/app/services/video_processing/opencv_processor.py` ‚Äì OpenCV Implementation (Zeile 116-274)
- `backend/app/services/s3_service.py` ‚Äì S3 Upload/Download (Zeile 38-200)
- `backend/worker/worker.py` ‚Äì Celery Worker (NICHT genutzt, Zeile 44-157)

### Log-Patterns f√ºr Monitoring
```regex
# Erfolgsfall
‚úÖ ENTERPRISE SUCCESS: \d+ frames extracted, duration=[\d.]+s

# Fehlerfall (Root Cause)
‚ùå ENTERPRISE FAILURE: METADATA_EXTRACTION_FAILED
‚ùå Could not open video with .+
‚ùå Could not extract metadata from .+ with any available backend

# Sekund√§r-Fehler
‚ùå FRAME EXTRACTION FAILED for .+
‚ùå AI vision analysis FAILED for .+: Frame extraction failed
```

### Kontakte & Verantwortlichkeiten
- **On-Call SRE**: [Slack @oncall, PagerDuty]
- **Backend Lead**: [GitHub @backend-lead]
- **ML Engineering**: [GitHub @ml-lead]
- **Render.com Dashboard**: https://dashboard.render.com/

---

**Ende des RCA-Berichts**  
*Erstellt: 2025-10-23 06:30 UTC*  
*Auditor-Signature: SRE-Lead-AI-2025*
